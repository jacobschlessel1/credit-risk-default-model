---
title: "Exploratory Data Analysis"
author: "Jacob Schlessel"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Purpose

This notebook performs exploratory data analysis for the accepted loan 
data.

## Setup

```{python}
import pandas as pd
import matplotlib.pyplot as plt

accepted = pd.read_parquet(
    "../data/processed/accepted_clean.parquet",
    engine="pyarrow"
)


plt.style.use("seaborn-v0_8")
```


## Exploring Loan Value

```{python}
import matplotlib.ticker as mtick


fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 10))

# Loan Amount
axes[0].hist(
    accepted["loan_amnt"],
    bins=40,
    edgecolor="black",
    linewidth=0.8
)
axes[0].set_title("Distribution of Loan Amount")
axes[0].set_xlabel("Loan Amount ($)")
axes[0].set_ylabel("Count")

# Interest Rate
axes[1].hist(
    accepted["int_rate"],
    bins=40,
    edgecolor="black",
    linewidth=0.8
)
axes[1].set_title("Distribution of Interest Rate")
axes[1].set_xlabel("Interest Rate (%)")
axes[1].set_ylabel("Count")

# Annual Income
upper = accepted["annual_inc"].quantile(0.999)
axes[2].hist(
    accepted["annual_inc"],
    bins=40,
    range=(0, upper),
    edgecolor="black",
    linewidth=0.8
)
axes[2].set_title("Distribution of Annual Income (≤ 99th Percentile)")
axes[2].set_xlabel("Annual Income ($)")
axes[2].set_ylabel("Count")

plt.tight_layout()
plt.show()
```

```{python}
vars = [
    "loan_amnt",
    "int_rate",
    "annual_inc"
]

percentiles = [0.25, 0.50, 0.75, 0.95, 0.99, 0.999]

summary_table = (
    accepted[vars]
    .describe(percentiles=percentiles)
    .loc[
        ["min", "25%", "50%", "75%", "95%", "99%", "99.9%", "max"]
    ]
    .T
)

summary_table
```

Loans were granted ranging from $1000 to $40,000, with 75% 
being less than $20,000. Interest rates varied from 4.85% all the way up 
to 30.89%, with most being in the range of roughly 10% to 16%. There were 
a quite a few outliers for `annual_inc`, with some invidiuals 
reporting over tens of millions of dollars in annual income (not shown on 
histogram), but most individuals made between 48,000 and $95,000 a year. 

Because there are millions of observations in the dataset, we will 
randomly sample 1,000 observations and create scatterplots to get a 
general sense of some relationships:

```{python}
# Create FICO avg
accepted["fico_avg"] = (
    accepted["fico_range_low"] + accepted["fico_range_high"]
) / 2

# Random sample of 1000
sample = accepted.sample(n=1000, random_state=42)

# Cap annual income for visualization 
inc_99 = accepted["annual_inc"].quantile(0.99)

fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))

# Loan Amount vs Annual Income
x = sample["annual_inc"].clip(upper=inc_99)
y = sample["loan_amnt"]

axes[0].scatter(x, y, alpha=0.4)
coef = np.polyfit(x, y, 1)
axes[0].plot(x, np.poly1d(coef)(x))

axes[0].set_title("Loan Amount vs Annual Income (Sample of 1,000)")
axes[0].set_xlabel("Annual Income ($)")
axes[0].set_ylabel("Loan Amount ($)")

# Loan Amount vs Interest Rate
x = sample["int_rate"]
y = sample["loan_amnt"]

coef = np.polyfit(x, y, 1)
axes[1].scatter(x, y, alpha=0.4)
axes[1].plot(x, np.poly1d(coef)(x))

axes[1].set_title("Loan Amount vs Interest Rate (Sample of 1,000)")
axes[1].set_xlabel("Interest Rate (%)")
axes[1].set_ylabel("Loan Amount ($)")

# Loan Amount vs Average FICO 
x = sample["fico_avg"]
y = sample["loan_amnt"]

coef = np.polyfit(x, y, 1)
axes[2].scatter(x, y, alpha=0.4)
axes[2].plot(x, np.poly1d(coef)(x))

axes[2].set_title("Loan Amount vs Average FICO Score (Sample of 1,000)")
axes[2].set_xlabel("Average FICO Score")
axes[2].set_ylabel("Loan Amount ($)")

plt.tight_layout()
plt.show()
```

There appears to be a positive relationship between loan amount and annual 
income, but no significant relationships between loan amount and either 
interest rate or average FICO score.

```{python}
import matplotlib.pyplot as plt

# Counts
term_counts = accepted["term"].value_counts().sort_index()
grade_counts = accepted["grade"].value_counts().sort_index()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))

# Loan term
axes[0].pie(
    term_counts.values,
    labels=term_counts.index.astype(str),
    autopct="%1.1f%%",
    startangle=90
)
axes[0].set_title("Loan Term Distribution (Months)")
axes[0].axis("equal")  

# Loan grade
axes[1].bar(
    grade_counts.index,
    grade_counts.values
)
axes[1].set_title("Loan Grade Distribution")
axes[1].set_xlabel("Grade")
axes[1].set_ylabel("Number of Loans")

plt.tight_layout()
plt.show()
```

Most of the loans were for 36 month terms rather than 60 months, and many 
of the loans were assigned grades of "C" or better upon lending.

## Geographic Distribution of Loans

```{python}
loans_by_region = (
    accepted
    .groupby("region")
    .size()
    .reset_index(name="n_loans")
    .sort_values("n_loans", ascending=False)
)

plt.figure(figsize=(7, 7))
plt.pie(
    loans_by_region["n_loans"],
    labels=loans_by_region["region"],
    autopct="%1.1f%%",
    startangle=90
)

plt.title("Share of Loans by Region")
plt.axis("equal")  
plt.tight_layout()
plt.show()
```

The southern US was the most common region of residence for borrowers, and 
the midwest was the least common region.

## Timing and Purpose of Loans

```{python}
loans_by_month = (
    accepted
    .set_index("issue_d")
    .resample("ME")
    .size()
)

plt.figure(figsize=(10, 5))

plt.plot(
    loans_by_month.index,
    loans_by_month.values
)

# vertical reference line
plt.axvline(
    pd.Timestamp("2016-01-01"),
    linestyle="--",
    linewidth=2,
    color="red",
    label="Began Tracking Credit Utilization Data"
)

plt.xlabel("Issue Date")
plt.ylabel("Number of Loans")
plt.title("Loan Originations per Month Over Time")

plt.legend()
plt.tight_layout()
plt.show()
```

The vertical red line indicates the point at which the credit utilization 
data began being tracked. In this dataset, the majority of loans are from 
2015 and onwards.

```{python}
top_5_purposes = (
    accepted["purpose"]
    .value_counts()
    .head(5)
    .index
)

loans_by_month_purpose = (
    accepted[accepted["purpose"].isin(top_5_purposes)]
    .groupby([pd.Grouper(key="issue_d", freq="ME"), "purpose"])
    .size()
    .reset_index(name="n_loans")
)

plt.figure(figsize=(11, 6))

for purpose, df_p in loans_by_month_purpose.groupby("purpose"):
    plt.plot(
        df_p["issue_d"],
        df_p["n_loans"],
        label=purpose
    )

plt.xlabel("Issue Date")
plt.ylabel("Number of Loans")
plt.title("Loan Originations Over Time by Purpose (Top 5)")
plt.legend(title="Purpose", bbox_to_anchor=(1.02, 1), loc="upper left")
plt.tight_layout()
plt.show()
```

Overall, loans trends were generally consistent over time regardless of 
purpose, with a notable exceptino being a spike in debt consolidiaton 
loans in mid 2017.

```{python}
df_box = accepted.loc[
    accepted["purpose"].isin(top_5_purposes),
    ["purpose", "int_rate"]
]

df_box.boxplot(
    column="int_rate",
    by="purpose",
    grid=False
)

plt.title("Interest Rate Distribution by Loan Purpose (Top 5)")
plt.suptitle("")  
plt.xlabel("Loan Purpose")
plt.ylabel("Interest Rate (%)")

plt.xticks(rotation=30)
plt.tight_layout()
plt.show()

from scipy.stats import kruskal

groups = [
    df_box.loc[df_box["purpose"] == p, "int_rate"]
    for p in top_5_purposes
]

h_stat, p_value_kw = kruskal(*groups)

print("Kruskal–Wallis Test: Interest Rate by Loan Purpose (Top 5)")
print("-" * 55)
print(f"H statistic : {h_stat:,.3f}")
print(f"p-value     : {p_value:.3e}")

if p_value < 0.05:
    print("Result      : Reject H₀ (distributions differ across purposes)")
else:
    print("Result      : Fail to reject H₀ (no evidence of differences)")
```

A quick Kruskal-Wallis test confirms that the distributions of interest 
rates are significantly different for each of the top 5 most common loan 
purposes. Debt consolidation loans tend to have slightly higher interest 
rates on average.

## Loan Outcomes

When analyzing loan outcomes, we need to only consider the subset of loans 
that are no longer current (as of when the data was collected).

```{python}
import seaborn as sns

df_violin = accepted.loc[
    accepted["current_flag"] == 0,
    ["annual_inc", "int_rate", "default", "loan_amnt"]
].dropna()

# Only include both 99%
inc_99 = df_violin["annual_inc"].quantile(0.99)

df_violin_plot = df_violin.copy()
df_violin_plot["annual_inc"] = (
  df_violin_plot["annual_inc"].clip(upper=inc_99))

fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))

# Annual Income
sns.violinplot(
    data=df_violin_plot,
    x="default",
    y="annual_inc",
    inner="quartile",
    cut=0,
    ax=axes[0]
)

axes[0].set_title("Annual Income by Default Status (≤ 99th Percentile)")
axes[0].set_xlabel("Default Status (0 = No Default, 1 = Default)")
axes[0].set_ylabel("Annual Income ($)")

# Interest rate
sns.violinplot(
    data=df_violin_plot,
    x="default",
    y="int_rate",
    inner="quartile",
    cut=0,
    ax=axes[1]
)

axes[1].set_title("Interest Rate by Default Status")
axes[1].set_xlabel("Default Status (0 = No Default, 1 = Default)")
axes[1].set_ylabel("Interest Rate (%)")


# Loan amount
sns.violinplot(
    data=df_violin_plot,
    x="default",
    y="loan_amnt",
    inner="quartile",
    cut=0,
    ax=axes[2]
)

axes[2].set_title("Loan Amount by Default Status")
axes[2].set_xlabel("Default Status (0 = No Default, 1 = Default)")
axes[2].set_ylabel("Loan Amount ($)")

plt.tight_layout()
plt.show()
```


Annual income follows a similar distribution for borrowers who defaulted 
versus those who did not, but the non-defaulted group has a slightly 
larger tail to the right. Non-defaulted loans appear to generally have 
lower interesed rates compared to loans that default. Similarly, it 
appears that defaulted loans were more frequently borrowing more money 
than non-defaulted loans.

```{python}

# Restrict to non-current loans
df_nc = accepted.loc[accepted["current_flag"] == 0].copy()

# Prepare variables

# grade
grade_counts = (
    df_nc
    .groupby(["grade", "default"])
    .size()
    .unstack(fill_value=0)
)
grade_props = grade_counts.div(grade_counts.sum(axis=1), axis=0)

# term
term_counts = (
    df_nc
    .groupby(["term", "default"])
    .size()
    .unstack(fill_value=0)
)
term_props = term_counts.div(term_counts.sum(axis=1), axis=0)

# purposes (top 5)
top_5_purposes = (
    df_nc["purpose"]
    .value_counts()
    .head(5)
    .index
)

purpose_counts = (
    df_nc.loc[df_nc["purpose"].isin(top_5_purposes)]
    .groupby(["purpose", "default"])
    .size()
    .unstack(fill_value=0)
)
purpose_props = purpose_counts.div(purpose_counts.sum(axis=1), axis=0)

# Plot
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))

plots = [
    (grade_counts, grade_props, "Default vs Non-Default by Loan Grade", "Loan Grade"),
    (term_counts, term_props, "Default vs Non-Default by Loan Term", "Loan Term (Months)"),
    (purpose_counts, purpose_props, "Default vs Non-Default by Loan Purpose (Top 5)", "Loan Purpose")
]

for ax, (counts, props, title, xlabel) in zip(axes, plots):

    counts.plot(kind="bar", stacked=True, ax=ax)

    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel("Number of Loans")
    ax.legend(["Non-Default", "Default"])
    ax.yaxis.set_major_formatter(mtick.StrMethodFormatter("{x:,.0f}"))

    # Add percentage labels
    for i, idx in enumerate(counts.index):
        cum_height = 0
        for d in [0, 1]:
            count = counts.loc[idx, d]
            prop = props.loc[idx, d]
            if count > 0:
                ax.text(
                    i,
                    cum_height + count / 2,
                    f"{prop:.0%}",
                    ha="center",
                    va="center",
                    fontsize=9,
                    color="white"
                )
            cum_height += count

    ax.tick_params(axis="x", rotation=30)

plt.tight_layout()
plt.show()
```

As the grade of the loan got worse, a higher proportion of borrowers 
defaulted, meaning the grades themselves have some predictive power. These 
however, will not be included in modeling as a predictor, since the 
formula was used to calculate these grade likely uses some of the other 
predictors already in in the dataset. Roughly double the proportion of 
borrowers defaulted on 60 month loans compared to 36 month loans. Debt 
consolidation was the most defaulted-on type of loan out of the five 
most common loan purposes.

## Hardship Outcomes

Similarly to default/non-default outcomes, we only want to consider 
non-current loans in exploring hardship outcomes because it is possible 
that current borrowers may still experience hardship before their loans 
are completed.

```{python}
df_hardship = accepted.loc[
    accepted["current_flag"] == 0,
    ["hardship_flag", "annual_inc", "loan_amnt", "emp_length", "dti", "pub_rec"]
].dropna()


# Restrict income, pubc_rec, and dti for visualizations
inc_99 = df_hardship["annual_inc"].quantile(0.99)
dti_99 = accepted["dti"].quantile(0.99)
pub_rec_99 = accepted["pub_rec"].quantile(0.99)


df_plot = df_hardship.copy()
df_plot["annual_inc"] = df_plot["annual_inc"].clip(upper=inc_99)
df_plot["dti"] = df_plot["dti"].clip(upper=dti_99)
df_plot["pub_rec"] = df_plot["pub_rec"].clip(upper=pub_rec_99)


fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(9, 16))

vars_and_labels = [
    ("annual_inc", "Annual Income (≤ 99th Percentile)"),
    ("loan_amnt", "Loan Amount"),
    ("emp_length", "Employment Length (Years)"),
    ("dti", "Debt-to-Income Ratio"),
    ("pub_rec", "Public Records")
]

for ax, (var, label) in zip(axes, vars_and_labels):
    sns.violinplot(
        data=df_plot,
        x="hardship_flag",
        y=var,
        inner="quartile",
        cut=0,
        color="red",
        ax=ax
    )

    ax.set_title(f"{label} by Hardship Status (Non-Current Loans)")
    ax.set_xlabel("Hardship Flag (0 = No Hardship, 1 = Hardship)")
    ax.set_ylabel(label)

plt.tight_layout()
plt.show()
```

The annual income, borrowed amount, and the debt-to-income ratios were 
similar among hardship and non-hardship borrowers, however those who 
experienced hardships generally had a higher number of public 
derogatory events, fewer years of consistent employment, 

```{python}
# Plot hardships over time for non-current loans
df_hardship = accepted.loc[
    (accepted["current_flag"] == 0) &
    (accepted["hardship_flag"] == 1) &
    (accepted["hardship_start_date"].notna())
]

hardships_by_month = (
    df_hardship
    .set_index("hardship_start_date")
    .resample("ME")
    .size()
)

plt.figure(figsize=(10, 5))

plt.plot(
    hardships_by_month.index,
    hardships_by_month.values
)

plt.xlabel("Hardship Start Date")
plt.ylabel("Number of Hardships")
plt.title("Number of Hardships per Month (Non-Current Loans)")

plt.tight_layout()
plt.show()
```

For non-current loans, the number of hardship periods spiked in October of 
2017.

## Loss After Borrowers Default

Loss given default (`lgd`) is defined as:

$$
\mathrm{LGD}
= 1 - \frac{\text{Total Recovered}}{\text{Exposure at Default (EAD)}}
$$

With the data from this dataset, this takes the following form:

$$
\mathrm{LGD}
= 1 - 
\frac{
\text{total\_rec\_prncp}
+ \text{total\_rec\_int}
+ \text{total\_rec\_late\_fee}
+ \text{recoveries}
- \text{collection\_recovery\_fee}
}{
\text{loan\_amnt}
}
$$



```{python}
# create lgd variable

# use only defaulted, non-current loans
df_lgd = accepted.loc[
    (accepted["default"] == 1) &
    (accepted["current_flag"] == 0)
].copy()


df_lgd["total_recovered"] = (
    df_lgd["total_rec_prncp"].fillna(0)
    + df_lgd["total_rec_int"].fillna(0)
    + df_lgd["total_rec_late_fee"].fillna(0)
    + df_lgd["recoveries"].fillna(0)
    - df_lgd["collection_recovery_fee"].fillna(0)
)

df_lgd["lgd"] = 1 - (df_lgd["total_recovered"] / df_lgd["loan_amnt"])
df_lgd["lgd"] = df_lgd["lgd"].clip(lower=0, upper=1)
```


```{python}
median_lgd_by_month = (
    df_lgd
    .set_index("issue_d")
    .resample("ME")["lgd"]
    .median()
)

plt.figure(figsize=(10, 5))

plt.plot(
    median_lgd_by_month.index,
    median_lgd_by_month.values
)

plt.xlabel("Issue Month")
plt.ylabel("Median LGD")
plt.title("Median LGD by Origination Month")
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))

plt.tight_layout()
plt.show()
```

Looking at this graph, `lgd` appears to increase as time goes on, however 
it is important to consider the context of the data. Loans that were 
issued more recently would be expected to have higher losses because the 
lender has had less time to initiate recovery operations, so over time 
the borrowers would return more of the loaned amount and the lender's 
losses would decrease until reaching they long-term loss amount (the true 
loss). For this project, only loans issued before 2016 will be considered 
for any loss given default analysis and modeling, as this ensures that 
the lender has had enough time to recover the money as of the 2019 
publication of the dataset.


```{python}
df_lgd_pre2016 = df_lgd.loc[
    df_lgd["issue_d"] < pd.Timestamp("2016-01-01")
].copy()

# compute median LGD by month
median_lgd_by_month = (
    df_lgd_pre2016
    .set_index("issue_d")
    .resample("ME")["lgd"]
    .median()
)

# plot
plt.figure(figsize=(10, 5))

plt.plot(
    median_lgd_by_month.index,
    median_lgd_by_month.values
)

plt.xlabel("Issue Month")
plt.ylabel("Median LGD")
plt.title("Median LGD by Origination Month (Loans Issued Before 2016)")
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))

plt.tight_layout()
plt.show()
```

LGD was slighly higher for loans issued in 2015 compared to the three 
years prior.

```{python}
plt.figure(figsize=(7, 5))

sns.violinplot(
    data=df_lgd_pre2016,
    x="hardship_flag",
    y="lgd",
    inner="quartile",
    cut=0
)

plt.title("LGD by Hardship Status, Pre-2016 Loans")
plt.xlabel("Hardship Flag (0 = No Hardship, 1 = Hardship)")
plt.ylabel("Loss Given Default (LGD)")
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))

plt.tight_layout()
plt.show()
```

Borrowers who both entered a hardsip plan and defaulted often resulted in 
lower losses for the lender on average than those who never entered a 
hardship plan at all. This could suggest that hardship plans are an 
effective strategy for lowering losses for the lender.



```{python}
median_lgd_by_grade = (
    df_lgd_pre2016
    .groupby("grade")["lgd"]
    .median()
    .sort_index()
)

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))

# LGD distribution
axes[0].hist(
    df_lgd_pre2016["lgd"],
    bins=40,
    edgecolor="black",
    linewidth=0.8
)

axes[0].set_title(
  "Distribution of Loss Given Default (LGD), Pre-2016 Loans")
axes[0].set_xlabel("LGD")
axes[0].set_ylabel("Number of Loans")
axes[0].xaxis.set_major_formatter(mtick.PercentFormatter(1.0))

# Median LGD by grade
axes[1].bar(
    median_lgd_by_grade.index,
    median_lgd_by_grade.values,
    edgecolor="black"
)

axes[1].set_title("Median LGD by Loan Grade, Pre-2016")
axes[1].set_xlabel("Loan Grade")
axes[1].set_ylabel("Median LGD")
axes[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))

plt.tight_layout()
plt.show()
```

Not all borrowers who default end up creating a loss for the lender, 
and loss generally tends to  increase as the grade of the loan worsens.