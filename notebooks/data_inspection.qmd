---
title: "Data Inspection â€“ LendingClub"
author: "Jacob Schlessel"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Purpose

This notebook performs initial data inspection and exploratory analysis
for the LendingClub credit risk dataset.

## Setup

```{python}
import pandas as pd
from pathlib import Path

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

# base data directory 
DATA_DIR = Path("../data/raw")

# read the two LendingClub datasets from S3 download
accepted = pd.read_csv(
    DATA_DIR / "accepted_2007_to_2018Q4.csv.gz",
    low_memory=False
)

# rejected loan data (load if needed later)
#rejected = pd.read_csv(
#    DATA_DIR / "rejected_2007_to_2018Q4.csv.gz",
#    low_memory=False
#)

accepted.shape
#rejected.shape
```

### Drop unnecessary columns from accepted
```{python}
accepted = accepted.drop(columns=["id", "member_id", "url"])
```


### Inspecting Missingness

```{python}
pd.options.display.float_format = "{:.4f}".format

# create function to print missingness for each category of variables
def print_missing(cols):

    missing_summary = (
        cols
        .isna()
        .agg(["sum", "mean"])
        .T
        .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
        .sort_values("prop_missing", ascending=False)
    )

    missing_summary["n_missing"] = missing_summary["n_missing"].astype(int)

    # do not truncate output
    with pd.option_context(
        "display.max_rows", None,
        "display.max_columns", None,
        "display.width", None,
        "display.float_format", "{:.4f}".format
    ):

        return missing_summary

print_missing(accepted)
```


As an initial observation, many rows have exactly 33 missing values, so 
we'll check if these are really just the same 33 rows that are without 
data.


```{python}
# check if all columns with exactly 33 missing values come from same rows
cols_33 = accepted.columns[accepted.isna().sum() == 33]
rows_33 = accepted[cols_33[0]].isna()
same_rows = accepted.loc[rows_33, cols_33].isna().all().all()

# print a preview of the rows with all these values missing and drop them
same_rows
```

They do in fact coincide, so we'll drop these rows.

```{python}
# drop rows with missing data
accepted = accepted.loc[~rows_33].copy()
```

#### Hardship Variables

```{python}
cols = [
    "orig_projected_additional_accrued_interest",
    "hardship_payoff_balance_amount",
    "payment_plan_start_date",
    "hardship_last_payment_amount",
    "hardship_status",
    "hardship_flag",
    "hardship_start_date",
    "deferral_term",
    "hardship_amount",
    "hardship_dpd",
    "hardship_loan_status",
    "hardship_length",
    "hardship_end_date",
    "hardship_reason",
    "hardship_type"
]

print_missing(accepted[cols])
```

The following hardship variables all have the same number of missing 
values:

- `harship_payoff_balance_amount`
- `payment_plan_start_date`
- `hardship_last_payment_amount`
- `hardship_status`
- `hardship_start_date`
- `deferral_term`
- `hardship_amount`
- `hardship_dpd`
- `hardship_loan_status`
- `hardship_length`
- `hardship_end_date`
- `hardship_reason`
- `hardship_type`

We'll check to make sure that there are all coincident.

```{python}
# check for coincidence
cols_hardship_missing = accepted.columns[accepted.isna().sum() == 2249751]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_hardship_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_hardship_missing].notna().all().all())
same_rows
```

These are all, in fact, coincident, meaning a missing value means an 
individual did not experience hardship, whereas present data means he/she 
did.


`orig_projected_additional_accrued_interest` has more missing values than 
the other hardship variables. This is either because this calculation was 
made at a later date than when the hardship period began, or it is because 
there are some hardship plans that freeze interest or only defer the 
principal.

```{python}
print(accepted["hardship_flag"].value_counts().sort_index())
```

There were 2,259,836 cases originally labelled as "No hardship" by the 
data curator but there were 2,249,751 instances of missing data for our 
hardship variables, meaning there are roughly 10,000 cases where a row 
has data for hardship variables but was classified as "No hardship". This 
difference can be explained by the fact that some hardship cases may not 
have formally approved or a hardship was cancelled early, so this project 
will fill `hardship_flag` as Y if there is data for these hardship 
variables and keep it as 'N' if there is no data (overruling original 
`hardship_flag` designation)

```{python}
import numpy as np

hardship_detail_cols = [
    "orig_projected_additional_accrued_interest",
    "hardship_payoff_balance_amount",
    "payment_plan_start_date",
    "hardship_last_payment_amount",
    "hardship_status",
    "hardship_start_date",
    "deferral_term",
    "hardship_amount",
    "hardship_dpd",
    "hardship_loan_status",
    "hardship_length",
    "hardship_end_date",
    "hardship_reason",
    "hardship_type"
]

# fill as Y if there is hardship data, keep as N if not
accepted["hardship_flag"] = np.where(
    accepted[hardship_detail_cols].notna().any(axis=1),
    "Y",
    "N"
)

print(accepted["hardship_flag"].value_counts().sort_index())
```

Now there are ~11,000 hardship cases, which will make modeling easier 
later on.

Hardship will not be a predictor 
variable for PD modeling. We are trying to predict a borrower's risk of 
defaulting on a loan at the time of applying, and because hardship is an 
event that occurs post-application, there would be severe leakage if this 
was included as a predictor. We will keep the individual hardship 
variables in the dataset to see if there are any macroeconomic conditions 
that increase risk of different types of hardship for the PH model. 
Additionally, hardship will be a predictor in the LGD models, as hardship 
is an event that occurs before defaulting.


#### Debt Settlement Variables

```{python}
cols = [
    "settlement_status",
    "debt_settlement_flag",
    "debt_settlement_flag_date",
    "settlement_date",
    "settlement_amount",
    "settlement_term",
    "settlement_percentage"
]

print_missing(accepted[cols])
```

Again, all of the debt settlement variables exlcuding 
`debt_settlement_flag` have the same number of missing values (2,226,422).
We will once again check for coincidence:

```{python}
# check for coincidence
cols_debt_missing = accepted.columns[accepted.isna().sum() == 2226422]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_debt_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_debt_missing].notna().all().all())
print(same_rows)
```

Here, missingness just means no settlement occurred, meaning the borrower 
did not ever default. Present data in these rows means that the borrower 
defaulted badly enough to negotiate a payment less than the amount that 
was originally owed.

```{python}
print(accepted["debt_settlement_flag"].value_counts().sort_index())
```

The number of "No settlements", matches the number of missing values for 
debt settlement variables, so the `debt_settlement_flag` variable is 
consistent with the data.

None of the debt settlement variables will be used in PD or PH modeling, 
as these are also post-application events (these even occur after 
deliquency). These variables will only be included in LGD models.

#### Secondary Applicant/ Joint Loan Variables

```{python}
cols = [
    "sec_app_mths_since_last_major_derog",
    "sec_app_revol_util",
    "revol_bal_joint",
    "sec_app_open_acc",
    "sec_app_num_rev_accts",
    "sec_app_inq_last_6mths",
    "sec_app_mort_acc",
    "sec_app_open_act_il",
    "sec_app_fico_range_low",
    "sec_app_fico_range_high",
    "sec_app_chargeoff_within_12_mths",
    "sec_app_collections_12_mths_ex_med",
    "sec_app_earliest_cr_line",
    "verification_status_joint",
    "dti_joint",
    "annual_inc_joint"
]

print_missing(accepted[cols])
```

The following secondary applicant/ joint loan variables all have the same number of missing values (2,152,647):

-`sec_app_revol_util`
-`revol_bal_joint`
-`sec_app_open_acc`
-`sec_app_num_rev_accts`
-`sec_app_inq_last_6mths`
-`sec_app_mort_acc`
-`sec_app_open_act_il`
-`sec_app_fico_range_low`
-`sec_app_fico_range_high`
-`sec_app_chargeoff_within_12_mths`
-`sec_app_collections_12_mths_ex_med`
-`sec_app_earliest_cr_line`

These variables have different numbers of missing values:
-`sec_app_mths_since_last_major_derog`
-`sec_app_revol_util`
-`revol_bal_joint`
-`verification_status_joint`
-`dti_joint`
-`annual_inc_joint`

Checking for coincidence for the first group:
```{python}
# check for coincidence
cols_joint_missing = accepted.columns[accepted.isna().sum() == 2152647]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_joint_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_joint_missing].notna().all().all())
print(same_rows)
```

For the first group, missingness indicates an individual loan. 
`sec_app_mths_since_last_major_derog` has more missing values than these 
variables, meaning there were some joint loans where the second applicant 
did not have a major derogatory credit event (~72,000). 
`sec_app_revol_util` has slightly more missing values (~1,800), meaning 
some joint applications have no revolving credit. 
`verification_status_joint` has slightly fewer missing values (~7,800 
less), meaning it has data even if `sec_app_*` does not have data, and the 
same goes for `dti_joint` and `annual_inc_joint` (~13,000 less each).


We fill a binary variable called `joint_flag` using the `application_type` 
variable, which is consistent with the data that exists for 
`annual_inc_joint`.


```{python}
# check number of joint applications labeled as such
print(accepted["application_type"].value_counts().sort_index())

# create joint_flag variable
accepted["joint_flag"] = (
accepted["application_type"] == "Joint App").astype(int)

# drop application_type (no longer needed)
accepted = accepted.drop(columns=["application_type"])
```

`joint_flag` will be used as a predictor for all of the PH, PD, and LGD 
models, while `annual_inc_joint`, `dti_joint`, and some of the `sec_app_*` 
variables will only be used as predictors in the LGD model.


#### Deliquency and Credit History Timing Variables

```{python}
cols = [
    "mths_since_last_record",
    "mths_since_recent_bc",
    "mths_since_recent_bc_dlq",
    "mths_since_recent_inq",
    "mths_since_last_major_derog",
    "mths_since_recent_revol_delinq",
    "mths_since_last_delinq",
    "delinq_2yrs",
    "collections_12_mths_ex_med",
    "acc_now_delinq",
    "chargeoff_within_12_mths",
    "delinq_amnt",
    "pub_rec_bankruptcies",
    "tax_liens"
]

print_missing(accepted[cols])
```

For each of these variables, missingness indicates that these events never 
occurred for that borrower:

-`mths_since_last_record` (~84% missing)
-`mths_since_recent_bc_dlq` (~78%)
-`mths_since_last_major_derog` (~74%)
-`mths_since_recent_revol_delinq`(~67%)
-`mths_since_last_delinq` (~51%)


These are based on the primary applicant's history, so any deliquencies 
that occurred for a secondary applicant appear under the joint loan 
variables.

We will create a flag variable for each of these columns (useful for 
linear modeling later), and we'll fill the missing values with a sentinel 
value of 999.

```{python}
mths_cols = [
    "mths_since_last_record",
    "mths_since_recent_bc_dlq",
    "mths_since_last_major_derog",
    "mths_since_recent_revol_delinq",
    "mths_since_last_delinq",
]

# create flags + fill sentinel value
for col in mths_cols:
    # flag: 1 if event ever occurred, 0 otherwise
    accepted[f"{col}_flag"] = accepted[col].notna().astype(int)
    
    # sentinel value for "never occurred"
    accepted[col] = accepted[col].fillna(999)
```

These variables, however, have present values for 0s but also have a small 
proportion of missing data:
- `collections_12_mths_ex_med` 
- `acc_now_delinq` 
- `chargeoff_within_12_mths` 
- `delinq_amnt` 
- `mths_since_recent_bc` 
- `mths_since_recent_inq` 
- `pub_rec_bankruptcies` 
- `tax_liens` 
- `delinq_2yrs`

Because there is such a small fraction of missing values for these, we will assume that missingness is just representative of "did not occur", so 
we'll fill these with 0s.

```{python}
cols = [
    "collections_12_mths_ex_med",
    "acc_now_delinq",
    "chargeoff_within_12_mths",
    "delinq_amnt",
    "mths_since_recent_bc",
    "mths_since_recent_inq",
    "pub_rec_bankruptcies",
    "tax_liens",
    "delinq_2yrs"
]

accepted[cols] = accepted[cols].fillna(0)
```

The deliquency variables will be used as predictors in for all PH, PD, and 
LGD models, with both the flag variables and actual values being used for 
all models (linear and non-linear). `delinq_2yrs`, `acc_now_delinq`
`collections_12_mths_ex_med`, `chargeoff_within_12_mths`, and 
`delinq_amnt` will remain as-is.

#### Borrower Description Field

```{python}
cols = ['desc']
print_missing(accepted[cols])
```

~94% of `desc` is missing, so we will just drop it from the dataset.

```{python}
accepted = accepted.drop(columns=["desc"])
```

#### Credit Utilization and Recent Credit Activity Variables

The following variables give enhanced information on borrower's credit:

```{python}
cols = [
    "il_util",
    "mths_since_rcnt_il",
    "all_util",
    "open_acc_6m",
    "inq_last_12m",
    "inq_last_6m",
    "total_cu_tl",
    "open_il_24m",
    "open_il_12m",
    "open_act_il",
    "max_bal_bc",
    "inq_fi",
    "total_bal_il",
    "open_rv_24m",
    "open_rv_12m"
]

print_missing(accepted[cols])
```

`il_util` is missing roughly 47% while the others are all missing around 
38%. Let's check for coincidence:

```{python}
# drop the one row where inq_last_6m is missing
accepted = accepted.loc[accepted["inq_last_6mths"].notna()].copy()

exclude = {"all_util", "il_util", "open_acc_6m", "inq_last_12m", "total_cu_tl", "mths_since_rcnt_il"}
cols_util_2 = [c for c in cols if c not in exclude]

# Rows where the FIRST variable is missing
rows_missing = accepted[cols_util_2[0]].isna()

# Check if all other variables are missing in exactly the same rows
all_coincident = accepted.loc[rows_missing, cols_util_2].isna().all().all()

print(all_coincident)
```

Missing values are all coincident (with the exception of one row for 
`open_acc_6m`, `inq_last_12m`, and `total_cu_tl` and 18 rows of 
`all_util`). Since `il_util` is missing more values while the others are 
all coincident, this means that not all borrowers have installment loans.

```{python}
# Drop one row with missing value for the 3 variables where others aren't
mask_three_missing = (
    accepted["open_acc_6m"].isna() &
    accepted["inq_last_12m"].isna() &
    accepted["total_cu_tl"].isna() &
    accepted[cols_util_2].notna().all(axis=1)
)

# Confirm it's exactly one row
mask_three_missing.sum()
accepted = accepted.loc[~mask_three_missing].copy()
```

The missing `all_util` values will be filled with 0s, as these omissions 
likely just represent an aggregate utilization rate of 0.
```{python}
accepted["all_util"] = accepted["all_util"].fillna(0)
```

We can check if the missing values of all the coincident variables are 
associated with when the loan application was filed:

```{python}
import matplotlib.pyplot as plt

# ensure issue_d is datetime, extract year
accepted["issue_d"] = pd.to_datetime(
    accepted["issue_d"],
    format="%b-%Y",
    errors="coerce"
)

accepted["issue_year"] = accepted["issue_d"].dt.year

# missingness rate
missing_rate_by_year = (
    accepted
    .groupby("issue_year")["inq_fi"]
    .apply(lambda x: x.isna().mean())
)

# loan volume
loan_counts = accepted.groupby("issue_year").size()

#plot
fig, ax1 = plt.subplots()

line1, = ax1.plot(
    missing_rate_by_year,
    marker="o",
    label="Missingness Rate"
)
ax1.set_ylabel("Missingness Rate")
ax1.set_xlabel("Issue Year")

ax2 = ax1.twinx()
line2, = ax2.plot(
    loan_counts,
    linestyle="--",
    label="Loan Count"
)
ax2.set_ylabel("Loan Count")

# legend
lines = [line1, line2]
labels = [line.get_label() for line in lines]
ax1.legend(
    lines,
    labels,
    loc="center left"
)

ax1.set_title(
    "Missingness Rate of Credit Utilization Variables vs Loan Volume by Year")
plt.show()
```

From this graph, we can see that most of the loans in the dataset were 
issued from the period of 2014-2018, and the credit utilization data was 
only collected from 2016 onwards. This insight will guide modeling 
decisions later.

#### Employment and Income Variables

```{python}
cols = ["emp_title", "emp_length", "annual_inc", "dti"]

print_missing(accepted[cols])
```

There are only 4 missing values for `annual_inc`, 113 for `dti`, and only 
6.5% of rows are missing `emp_length`. We will safely drop the rows where 
either of the two are missing, as our sample size is still large enough 
and there is such a small proportion of observations missing these values. 
We will also drop the `emp_title` variable entirely, as it will not be 
useful for modeling.

```{python}
# drop emp_title column
accepted = accepted.drop(columns=["emp_title"])

# drop rows where emp_length is missing
accepted = accepted.dropna(subset=["emp_length"])

# drop rows where annual_inc is missing
accepted = accepted.dropna(subset=["annual_inc"])

# drop rows where dti is missing
accepted = accepted.dropna(subset=["dti"])
```

#### Credit Account Counts and Balances

```{python}
cols = [
    "open_acc",
    "pub_rec",
    "revol_bal",
    "revol_util",
    "total_acc",
    "num_tl_120dpd_2m",
    "mo_sin_old_il_acct",
    "bc_util",
    "percent_bc_gt_75",
    "bc_open_to_buy",
    "mths_since_recent_bc",
    "pct_tl_nvr_dlq",
    "avg_cur_bal",
    "mo_sin_rcnt_rev_tl_op",
    "num_rev_accts",
    "mo_sin_old_rev_tl_op",
    "num_op_rev_tl",
    "tot_coll_amt",
    "num_actv_rev_tl",
    "total_rev_hi_lim",
    "tot_cur_bal",
    "num_tl_30dpd",
    "num_actv_bc_tl",
    "num_accts_ever_120_pd",
    "num_rev_tl_bal_gt_0",
    "mo_sin_rcnt_tl",
    "num_tl_90g_dpd_24m",
    "tot_hi_cred_lim",
    "total_il_high_credit_limit",
    "num_il_tl",
    "num_tl_op_past_12m",
    "num_bc_tl",
    "num_bc_sats",
    "num_sats",
    "total_bal_ex_mort",
    "mort_acc",
    "acc_open_past_24mths",
    "total_bc_limit"
]

print_missing(accepted[cols])
```

For `num_tl_120dpd_2m` and `mo_sin_old_il_acct`, missingness indicates 
0 accounts overdue and no prior installment loan, respectively. We can 
missing values with 0 for the former, and we'll create a flag variable 
for the latter while also filling missing values with a neutral value. 
This preserves these observations while maintaining interpretability 
for modeling. We can also fill missing `revol_util` values with 0, as this 
indicates the borrower has no revolving credit accounts.

```{python}
accepted["num_tl_120dpd_2m"] = (
    accepted["num_tl_120dpd_2m"]
    .fillna(0)
)

# create flag variable for presence/absence of prior installment loan
accepted["mo_sin_old_il_acct_missing"] = (
    accepted["mo_sin_old_il_acct"].isna().astype(int)
)

# fill missing values with neutral value of 0
accepted["mo_sin_old_il_acct"] = (
    accepted["mo_sin_old_il_acct"]
    .fillna(0)
)

# fill missing revol_util with 0s
accepted["revol_util"] = accepted["revol_util"].fillna(0)
```

We'll check if the missing values of all the variables with exactly 68,211 
missing values are conincident.

```{python}
# columns with exactly 68,211 missing values
cols_68211 = accepted.isna().sum().eq(68211)

# subset to those columns
cols_68211 = cols_68211[cols_68211].index

# check for coincidence
is_coincident = accepted[cols_68211].isna().all(axis=1).sum() == 68211

print(is_coincident)
```

The missingness of these variables is coincident, and since they only make 
up ~3% of observations, we will drop these from the dataset for simplicity.

```{python}
cols_68211 = accepted.isna().sum().eq(68211)
cols_68211 = cols_68211[cols_68211].index

# Drop rows where all of these variables are missing
accepted = accepted.dropna(subset=cols_68211)
```


For the remaining variables, missingness can be explained in the 
following ways:

-`bc_util`: no bankcard, can be filled with 0s
-`percent_bc_gt_75`: no bankcard, can be filled with 0s
-`bc_open_to_buy`: no bankcard, can be filled with 0s
-`mths_since_recent_bc`: no bankcard, can be filled with 0s
-`pct_tl_nvr_dlq`: data collection error, can be dropped 
-`avg_cur_bal`: data collection error, can be dropped 
-`num_rev_accts`: data collection error, can be dropped 

```{python}
# fill 0s for no bankcard variables
credit_zero_cols = [
    "bc_util",
    "percent_bc_gt_75",
    "bc_open_to_buy",
    "mths_since_recent_bc"
]
accepted[credit_zero_cols] = accepted[credit_zero_cols].fillna(0)

# drop rows for data collection errors
accepted = accepted.dropna(
    subset=["pct_tl_nvr_dlq", "avg_cur_bal", "num_rev_accts"]
)
```

#### Payment and Outcome Variables (Post-Origination)

```{python}
cols = [
    "last_pymnt_d",
    "next_pymnt_d",
    "last_pymnt_amnt",
    "recoveries",
    "collection_recovery_fee",
    "total_rec_prncp",
    "total_rec_int",
    "total_rec_late_fee",
    "out_prncp"
]

print_missing(accepted[cols])
```

Where `next_pyment_d` is missing, this indicates a completed loan. 
`last_pyment_d` missing likely indicates a newly issued load (as of the 
publication of the data), so these will be dropped since they will not be 
useful for modeling. The rest of the payment variables do not have any 
missing values.

```{python}
# drop rows where next and last payment is missing
accepted = accepted.drop(columns=["next_pymnt_d", "last_pymnt_d"])
```

#### Administrative and Origination Variables

```{python}
cols = [
    "zip_code",
    "loan_amnt",
    "funded_amnt",
    "funded_amnt_inv",
    "term",
    "purpose",
    "issue_d",
    "loan_status",
    "addr_state"
]

print_missing(accepted[cols])
```

`zip_code` will not be useful for modeling, so we can drop this column. We 
should look at the status of the loans in the dataset:

```{python}
accepted = accepted.drop(columns=["zip_code"])
accepted["loan_status"].value_counts().sort_index()
```

We will create a binary variable called `current_flag` to classify loans 
as current (1) or finished (0), meaning it was paid off or the borrower 
defaulted.

```{python}
# list of active statuses
active_statuses = [
    "Current",
    "In Grace Period",
    "Late (16-30 days)",
    "Late (31-120 days)"
]

# current = 1 if active, 0 if not
accepted["current_flag"] = (
    accepted["loan_status"].isin(active_statuses).astype(int))

```

We will also create a variable called `default`, where 1 means the 
borrower either defaulted or the loan was charged off, and 0 means 
the loan was either paid back in full or is still active (including late 
or in grace period).

```{python}
default_statuses = ["Charged Off", "Default"]

accepted["default"] = (
    accepted["loan_status"].isin(default_statuses).astype(int))
```

Lastly, we will map each state to a region to lower its cardinality for 
modeling.

```{python}
state_to_region = {
    # Northeast
    "CT": "Northeast", "ME": "Northeast", "MA": "Northeast", "NH": "Northeast",
    "RI": "Northeast", "VT": "Northeast", "NJ": "Northeast", "NY": "Northeast",
    "PA": "Northeast",

    # Midwest
    "IL": "Midwest", "IN": "Midwest", "MI": "Midwest", "OH": "Midwest",
    "WI": "Midwest", "IA": "Midwest", "KS": "Midwest", "MN": "Midwest",
    "MO": "Midwest", "NE": "Midwest", "ND": "Midwest", "SD": "Midwest",

    # South
    "DE": "South", "FL": "South", "GA": "South", "MD": "South",
    "NC": "South", "SC": "South", "VA": "South", "WV": "South",
    "AL": "South", "KY": "South", "MS": "South", "TN": "South",
    "AR": "South", "LA": "South", "OK": "South", "TX": "South",
    "DC": "South",

    # West
    "AZ": "West", "CO": "West", "ID": "West", "MT": "West",
    "NV": "West", "NM": "West", "UT": "West", "WY": "West",
    "AK": "West", "CA": "West", "HI": "West", "OR": "West", "WA": "West"
}

# Map and drop addr_state
accepted["region"] = accepted["addr_state"].map(state_to_region)
accepted = accepted.drop(columns=["addr_state"])
```

#### Loan Pricing and Risk Grade Variables

```{python}
cols = [
    "int_rate",
    "installment",
    "grade",
    "sub_grade",
    "initial_list_status",
    "pymnt_plan"
]

print_missing(accepted[cols])
```

There are not any missing values for the loan pricing variables.

#### Borrower Credit Score Variables

```{python}
cols = [
    "fico_range_low",
    "fico_range_high",
    "earliest_cr_line"
]

print_missing(accepted[cols])
```

There are also not any missing values for the credit score variables.

## Data Cleaning

```{python}
# change term to numbers only
accepted["term"] = (
    accepted["term"]
    .str.replace(" months", "", regex=False)
    .astype(int)
)

# Change emp_length to numbers only
accepted["emp_length"] = (
    accepted["emp_length"]
    .str.replace("10\\+ years", "10", regex=True)
    .str.replace("< 1 year", "0", regex=False)
    .str.replace(" years", "", regex=False)
    .str.replace(" year", "", regex=False)
)
accepted["emp_length"] = accepted["emp_length"].astype(int)


# Create verificaiton flag variable, drop verification_status
accepted["verified_flag"] = (
    accepted["verification_status"]
    .isin(["Verified", "Source Verified"])
    .astype(int)
)
accepted = accepted.drop(columns=["verification_status"])

# Drop title column
accepted = accepted.drop(columns=["title"])

# Make earliest_cr_line a datetime
accepted["earliest_cr_line"] = pd.to_datetime(
    accepted["earliest_cr_line"],
    format="%b-%Y",
    errors="coerce"
)

# encode initial_list_status as 1 (whole) or 0 (fractionalized)
accepted["whole_loan_flag"] = (accepted["initial_list_status"] == "w").astype(int)
accepted = accepted.drop(columns=["initial_list_status"])

# drop out_prncp_inv, total_pyment, total_pyment_inv
accepted = accepted.drop(
    columns=["out_prncp_inv",
    "total_pymnt",
    "total_pymnt_inv"])

# drop post-origination credit check variables
accepted = accepted.drop(
    columns=["last_credit_pull_d", "last_fico_range_low", "last_fico_range_high"]
)

# drop policy code column
accepted = accepted.drop(columns=["policy_code"])

# create verification flag JOINT variable, drop verification_status_joint
accepted["verified_flag_joint"] = (
    accepted["verification_status_joint"]
    .isin(["Verified", "Source Verified"])
    .astype(int)
)
accepted = accepted.drop(columns=["verification_status_joint"])

# Make sec_app_earliest_cr_line a datetime
accepted["sec_app_earliest_cr_line"] = pd.to_datetime(
    accepted["earliest_cr_line"],
    format="%b-%Y",
    errors="coerce"
)

# Change hardship_flag to binary (1 = hardship, 0 = no hardship)
accepted["hardship_flag"] = (accepted["hardship_flag"] == "Y").astype(int)

# Drop hardship_type
accepted = accepted.drop(columns=["hardship_type"])

# Drop hardship_length -> always 3 months
accepted = accepted.drop(columns=["hardship_length"])

# Drop disbursement_method
accepted = accepted.drop(columns=["disbursement_method"])

# change date cols to datetime
date_cols = [
    "hardship_start_date",
    "hardship_end_date",
    "payment_plan_start_date",
    "debt_settlement_flag_date",
    "settlement_date"
]

for col in date_cols:
    accepted[col] = pd.to_datetime(
        accepted[col],
        format="%b-%Y",
        errors="coerce"
    )


# Change debt_settlement_flag to binary (1 = settlement, 0 = nonee)
accepted["debt_settlement_flag"] = (
    accepted["debt_settlement_flag"] == "Y").astype(int)
```

