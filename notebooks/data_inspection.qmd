---
title: "Data Inspection â€“ LendingClub"
author: "Jacob Schlessel"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Purpose

This notebook performs initial data inspection and exploratory analysis
for the LendingClub credit risk dataset.

## Setup

```{python}
import pandas as pd
from pathlib import Path

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

# base data directory 
DATA_DIR = Path("../data/raw")

# read the two LendingClub datasets from S3 download
accepted = pd.read_csv(
    DATA_DIR / "accepted_2007_to_2018Q4.csv.gz",
    low_memory=False
)

rejected = pd.read_csv(
    DATA_DIR / "rejected_2007_to_2018Q4.csv.gz",
    low_memory=False
)

accepted.shape, rejected.shape
```

### Drop unnecessary columns from accepted and rejected
```{python}
accepted = accepted.drop(columns=["id", "member_id", "url"])
```


### Inspecting Missingness

```{python}
missing_summary = (
    accepted
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

missing_summary
```


Many rows have exactly 33 missing values, so we'll check if these are 
really just the same 33 rows that are without data.


```{python}
# check if all columns with exactly 33 missing values come from same rows
cols_33 = accepted.columns[accepted.isna().sum() == 33]
rows_33 = accepted[cols_33[0]].isna()
same_rows = accepted.loc[rows_33, cols_33].isna().all().all()

# print a preview of the rows with all these values missing and drop them
accepted.loc[rows_33].head()
accepted = accepted.loc[~rows_33].copy()
```

#### Hardship Variables

The following hardship variables all have the same number of missing 
values:

- `harship_payoff_balance_amount`
- `payment_plan_start_date`
- `hardship_last_payment_amount`
- `hardship_status`
- `hardship_start_date`
- `deferral_term`
- `hardship_amount`
- `hardship_dpd`
- `hardship_loan_status`
- `hardship_length`
- `hardship_end_date`
- `hardship_reason`
- `hardship_type`

We'll check to make sure that there are all coincident.

```{python}
# check for coincidence
cols_hardship_missing = accepted.columns[accepted.isna().sum() == 2249751]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_hardship_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_hardship_missing].notna().all().all())
print(same_rows)
```

These are all, in fact, coincident, meaning a missing value means an 
individual did not experience hardship, whereas present data means he/she 
did.


`orig_projected_additional_accrued_interest` has more missing values than 
the other hardship variables. This is either because this calculation was 
made at a later date than when the hardship period began, or it is because 
there are some hardship plans that freeze interest or only defer the 
principal.

```{python}
print(accepted["hardship_flag"].value_counts().sort_index())
```

There were 2,259,836 cases originally labelled as "No hardship" by the 
data curator but there were 2,249,751 instances of missing data for our 
hardship variables, meaning there are roughly 10,000 cases where a row 
has data for hardship variables but was classified as "No hardship". This 
difference can be explained by the fact that some hardship cases may not 
have formally approved or a hardship was cancelled early, so this project 
will trust that the official designations for these cases is correct and 
will not fill `hardship_flag` based on missing/nonmissing data.


Hardship will be an outcome variable and not a predictor 
variable for PD modeling. We are trying to model a borrower's risk of 
defaulting on a loan at the time of applying, and because hardship is an 
event that occurs post-application, there would be severe leakage if this 
was included as a predictor. We will keep the individual hardship 
variables in the dataset to see if there are any macroeconomic conditions 
that increase risk of different types of hardship for the PH model. 
Additionally, hardship will be a predictor in the LGD models.


#### Debt Settlement Variables

Again, there are the exact same number of missing values for all of the 
debt settlement variables (2,226,422):

-`settlement_status`
-`debt_settlement_flag_date`
-`settlement_date`
-`settlement_amount`
-`settlement_term`
-`settlement_percentage`

We will once again check for coincidence:

```{python}
# check for coincidence
cols_debt_missing = accepted.columns[accepted.isna().sum() == 2226422]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_debt_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_debt_missing].notna().all().all())
print(same_rows)
```

Here, missingness just means no settlement occurred, meaning the borrower 
did not default at all. Present data in these rows means that the borrower 
defaulted badly enough to negotiate a payment less than the amount that 
was originally owed.

```{python}
print(accepted["debt_settlement_flag"].value_counts().sort_index())
```

The number of "No settlements", matches the number of missing values for 
debt settlement variables, so the `debt_settlement_flag` variable is 
consistent with the data.

None of the debt settlement variables will be used in PD or PH modeling, 
as these are also post-application events (these even occur after 
deliquency). These variables will only be included in LGD models.

#### Secondary Applicant/ Joint Loan Variables

The following secondary applicant/ joint loan variables all have the same number of missing values (2,152,647):

-`sec_app_revol_util`
-`revol_bal_joint`
-`sec_app_open_acc`
-`sec_app_num_rev_accts`
-`sec_app_inq_last_6mths`
-`sec_app_mort_acc`
-`sec_app_open_act_il`
-`sec_app_fico_range_low`
-`sec_app_fico_range_high`
-`sec_app_chargeoff_within_12_mths`
-`sec_app_collections_12_mths_ex_med`
-`sec_app_earliest_cr_line`

These three variables have different numbers of missing values:
-`sec_app_mths_since_last_major_derog`
-`sec_app_revol_util`
-`revol_bal_joint`
-`verification_status_joint`
-`dti_joint`
-`annual_inc_joint`

Checking for coincident for the first group:
```{python}
# check for coincidence
cols_joint_missing = accepted.columns[accepted.isna().sum() == 2152647]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_joint_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_joint_missing].notna().all().all())
print(same_rows)
```

For the first group, missingness indicates an individual loan. 
`sec_app_mths_since_last_major_derog` has more missing values than these 
variables, meaning there were some joint loans where the second applicant 
did not have a major derogatory credit event (~72,000). 
`sec_app_revol_util` has slightly more missing values (~1,800), meaning 
some joint applications have no revolving credit. 
`verification_status_joint` has slightly fewer missing values (~7,800 
less), meaning it has data even if `sec_app_*` does not have data, and the 
same goes for `dti_joint` and `annual_inc_joint` (~13,000 less each).


We fill a binary variable called `joint_flag` using the `application_type` 
variable, which is consistent with the data that exists for 
`annual_inc_joint`.


```{python}
# check number of joint applications labeled as such
print(accepted["application_type"].value_counts().sort_index())

# create joint_flag variable
accepted["joint_flag"] = (
accepted["application_type"] == "Joint App").astype(int)

# drop application_type (no longer needed)
accepted = accepted.drop(columns=["application_type"])
```

#### Deliquency and Credit History Timing Variables

For each of these variables, missingness indicates that these events never 
occurred for that borrower:

-`mths_since_last_record` (~84% missing)
-`mths_since_recent_bc_dlq` (~78%)
-`mths_since_last_major_derog` (~74%)
-`mths_since_recent_revol_delinq`(~67%)
-`mths_since_last_delinq` (~51%)

These are based on the primary applicant's history, so any deliquencies 
that occurred for a secondary applicant appear under the joint loan 
variables.

We will create a flag variable for each of these columns (useful for 
linear modeling later), and we'll fill the missing values with a sentinel 
value of 999.

```{python}
mths_cols = [
    "mths_since_last_record",
    "mths_since_recent_bc_dlq",
    "mths_since_last_major_derog",
    "mths_since_recent_revol_delinq",
    "mths_since_last_delinq"
]

# create flags + fill sentinel value
for col in mths_cols:
    # flag: 1 if event ever occurred, 0 otherwise
    accepted[f"{col}_flag"] = accepted[col].notna().astype(int)
    
    # sentinel value for "never occurred"
    accepted[col] = accepted[col].fillna(999)
```



## Data Cleaning

```{python}
accepted["term"] = (
    accepted["term"]
    .str.replace(" months", "", regex=False)
    .astype(int)
)

accepted["emp_length"] = (
    accepted["emp_length"]
    .str.replace("+ years", "", regex=False)
    .astype(int)
)
```


```{python}
accepted["hardship_status"].value_counts(dropna=False).sort_index()
```