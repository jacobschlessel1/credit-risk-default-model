---
title: "Data Inspection â€“ LendingClub"
author: "Jacob Schlessel"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Purpose

This notebook performs initial data inspection and exploratory analysis
for the LendingClub credit risk dataset.

## Setup

```{python}
import pandas as pd
from pathlib import Path

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

# base data directory 
DATA_DIR = Path("../data/raw")

# read the two LendingClub datasets from S3 download
accepted = pd.read_csv(
    DATA_DIR / "accepted_2007_to_2018Q4.csv.gz",
    low_memory=False
)

#comment out for now
#rejected = pd.read_csv(
#    DATA_DIR / "rejected_2007_to_2018Q4.csv.gz",
#    low_memory=False
#)

accepted.shape, rejected.shape
```

### Drop unnecessary columns from accepted and rejected
```{python}
accepted = accepted.drop(columns=["id", "member_id", "url"])
```


### Inspecting Missingness

```{python}
pd.options.display.float_format = "{:.4f}".format

missing_summary = (
    accepted
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

missing_summary
```


Many rows have exactly 33 missing values, so we'll check if these are 
really just the same 33 rows that are without data.


```{python}
# check if all columns with exactly 33 missing values come from same rows
cols_33 = accepted.columns[accepted.isna().sum() == 33]
rows_33 = accepted[cols_33[0]].isna()
same_rows = accepted.loc[rows_33, cols_33].isna().all().all()

# print a preview of the rows with all these values missing and drop them
accepted.loc[rows_33].head()
accepted = accepted.loc[~rows_33].copy()
```

#### Hardship Variables

The following hardship variables all have the same number of missing 
values:

- `harship_payoff_balance_amount`
- `payment_plan_start_date`
- `hardship_last_payment_amount`
- `hardship_status`
- `hardship_start_date`
- `deferral_term`
- `hardship_amount`
- `hardship_dpd`
- `hardship_loan_status`
- `hardship_length`
- `hardship_end_date`
- `hardship_reason`
- `hardship_type`

We'll check to make sure that there are all coincident.

```{python}
# check for coincidence
cols_hardship_missing = accepted.columns[accepted.isna().sum() == 2249751]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_hardship_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_hardship_missing].notna().all().all())
print(same_rows)
```

These are all, in fact, coincident, meaning a missing value means an 
individual did not experience hardship, whereas present data means he/she 
did.


`orig_projected_additional_accrued_interest` has more missing values than 
the other hardship variables. This is either because this calculation was 
made at a later date than when the hardship period began, or it is because 
there are some hardship plans that freeze interest or only defer the 
principal.

```{python}
print(accepted["hardship_flag"].value_counts().sort_index())
```

There were 2,259,836 cases originally labelled as "No hardship" by the 
data curator but there were 2,249,751 instances of missing data for our 
hardship variables, meaning there are roughly 10,000 cases where a row 
has data for hardship variables but was classified as "No hardship". This 
difference can be explained by the fact that some hardship cases may not 
have formally approved or a hardship was cancelled early, so this project 
will trust that the official designations for these cases is correct and 
will not fill `hardship_flag` based on missing/nonmissing data.


Hardship will be an outcome variable and not a predictor 
variable for PD modeling. We are trying to model a borrower's risk of 
defaulting on a loan at the time of applying, and because hardship is an 
event that occurs post-application, there would be severe leakage if this 
was included as a predictor. We will keep the individual hardship 
variables in the dataset to see if there are any macroeconomic conditions 
that increase risk of different types of hardship for the PH model. 
Additionally, hardship will be a predictor in the LGD models.


#### Debt Settlement Variables

Again, there are the exact same number of missing values for all of the 
debt settlement variables (2,226,422):

-`settlement_status`
-`debt_settlement_flag_date`
-`settlement_date`
-`settlement_amount`
-`settlement_term`
-`settlement_percentage`

We will once again check for coincidence:

```{python}
# check for coincidence
cols_debt_missing = accepted.columns[accepted.isna().sum() == 2226422]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_debt_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_debt_missing].notna().all().all())
print(same_rows)
```

Here, missingness just means no settlement occurred, meaning the borrower 
did not default at all. Present data in these rows means that the borrower 
defaulted badly enough to negotiate a payment less than the amount that 
was originally owed.

```{python}
print(accepted["debt_settlement_flag"].value_counts().sort_index())
```

The number of "No settlements", matches the number of missing values for 
debt settlement variables, so the `debt_settlement_flag` variable is 
consistent with the data.

None of the debt settlement variables will be used in PD or PH modeling, 
as these are also post-application events (these even occur after 
deliquency). These variables will only be included in LGD models.

#### Secondary Applicant/ Joint Loan Variables

The following secondary applicant/ joint loan variables all have the same number of missing values (2,152,647):

-`sec_app_revol_util`
-`revol_bal_joint`
-`sec_app_open_acc`
-`sec_app_num_rev_accts`
-`sec_app_inq_last_6mths`
-`sec_app_mort_acc`
-`sec_app_open_act_il`
-`sec_app_fico_range_low`
-`sec_app_fico_range_high`
-`sec_app_chargeoff_within_12_mths`
-`sec_app_collections_12_mths_ex_med`
-`sec_app_earliest_cr_line`

These three variables have different numbers of missing values:
-`sec_app_mths_since_last_major_derog`
-`sec_app_revol_util`
-`revol_bal_joint`
-`verification_status_joint`
-`dti_joint`
-`annual_inc_joint`

Checking for coincident for the first group:
```{python}
# check for coincidence
cols_joint_missing = accepted.columns[accepted.isna().sum() == 2152647]

# identify rows where the FIRST such column is NON-missing
rows_non_missing = accepted[cols_joint_missing[0]].notna()

# check if ALL other columns are also non-missing in EXACTLY those rows
same_rows = (
accepted.loc[rows_non_missing, cols_joint_missing].notna().all().all())
print(same_rows)
```

For the first group, missingness indicates an individual loan. 
`sec_app_mths_since_last_major_derog` has more missing values than these 
variables, meaning there were some joint loans where the second applicant 
did not have a major derogatory credit event (~72,000). 
`sec_app_revol_util` has slightly more missing values (~1,800), meaning 
some joint applications have no revolving credit. 
`verification_status_joint` has slightly fewer missing values (~7,800 
less), meaning it has data even if `sec_app_*` does not have data, and the 
same goes for `dti_joint` and `annual_inc_joint` (~13,000 less each).


We fill a binary variable called `joint_flag` using the `application_type` 
variable, which is consistent with the data that exists for 
`annual_inc_joint`.


```{python}
# check number of joint applications labeled as such
print(accepted["application_type"].value_counts().sort_index())

# create joint_flag variable
accepted["joint_flag"] = (
accepted["application_type"] == "Joint App").astype(int)

# drop application_type (no longer needed)
accepted = accepted.drop(columns=["application_type"])
```

`joint_flag` will be used as a predictor for all of the PH, PD, and LGD 
models, while `annual_inc_joint`, `dti_joint`, and some of the `sec_app_*` 
variables will only be used as predictors in the LGD model.


#### Deliquency and Credit History Timing Variables

For each of these variables, missingness indicates that these events never 
occurred for that borrower:

-`mths_since_last_record` (~84% missing)
-`mths_since_recent_bc_dlq` (~78%)
-`mths_since_last_major_derog` (~74%)
-`mths_since_recent_revol_delinq`(~67%)
-`mths_since_last_delinq` (~51%)

These are based on the primary applicant's history, so any deliquencies 
that occurred for a secondary applicant appear under the joint loan 
variables.

We will create a flag variable for each of these columns (useful for 
linear modeling later), and we'll fill the missing values with a sentinel 
value of 999.

```{python}
mths_cols = [
    "mths_since_last_record",
    "mths_since_recent_bc_dlq",
    "mths_since_last_major_derog",
    "mths_since_recent_revol_delinq",
    "mths_since_last_delinq"
]

# create flags + fill sentinel value
for col in mths_cols:
    # flag: 1 if event ever occurred, 0 otherwise
    accepted[f"{col}_flag"] = accepted[col].notna().astype(int)
    
    # sentinel value for "never occurred"
    accepted[col] = accepted[col].fillna(999)
```

The deliquency variables will be used as predictors in for all PH, PD, and 
LGD models, with both the flag variables and actual values being used for 
all models (linear and non-linear).

#### Borrower Description Field

~94% of `desc` is missing, so we will just drop it from the dataset.

```{python}
accepted = accepted.drop(columns=["desc"])
```

#### Credit Utilization and Recent Credit Activity Variables

The following variables give enhanced information on borrower's credit:

-`il_util`
-`all_util`
-`open_acc_6m`
-`inq_last_12m`
-`total_cu_tl`
-`open_il_24m`
-`open_il_12m`
-`open_act_il`
-`max_bal_bc`
-`inq_fi`
-`total_bal_il`
-`open_rv_24m`
-`open_rv_12m`

```{python}
# create list of util variables
cols_util = [
    "il_util",
    "all_util",
    "open_acc_6m",
    "inq_last_12m",
    "total_cu_tl",
    "open_il_24m",
    "open_il_12m",
    "open_act_il",
    "max_bal_bc",
    "inq_fi",
    "total_bal_il",
    "open_rv_24m",
    "open_rv_12m"
]

# print missingness
missing_summary = (
    accepted[cols_util]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "missing_count", "mean": "missing_proportion"})
)

print(missing_summary)
```

`il_util` is missing roughly 47% while the others are all missing around 
38%. Let's check for coincidence:

```{python}
exclude = {"all_util", "il_util", "open_acc_6m", "inq_last_12m", "total_cu_tl"}
cols_util_2 = [c for c in cols_util if c not in exclude]

# Rows where the FIRST variable is missing
rows_missing = accepted[cols_util_2[0]].isna()

# Check if all other variables are missing in exactly the same rows
all_coincident = accepted.loc[rows_missing, cols_util_2].isna().all().all()

print(all_coincident)
```

Missing values are all coincident (with the exception of one row for 
`open_acc_6m`, `inq_last_12m`, and `total_cu_tl`). Since `il_util` is 
missing more values while the others are all coincident, this means that not all borrowers have installment loans.

```{python}
# Drop one row with missing value for the 3 variables where others aren't
drop_mask = (
    accepted["open_acc_6m"].isna() &
    accepted["inq_last_12m"].isna() &
    accepted["total_cu_tl"].isna() &
    accepted[cols_util_2].notna().all(axis=1)
)

# Confirm it's exactly one row
drop_mask.sum()
accepted = accepted.loc[~drop_mask].copy()
```

We can check if the missing values are associated with when the loan 
application was filed:

```{python}
import matplotlib.pyplot as plt

# ensure issue_d is datetime, extract year
accepted["issue_d"] = pd.to_datetime(accepted["issue_d"])
accepted["issue_year"] = accepted["issue_d"].dt.year

# missingness rate
missing_rate_by_year = (
    accepted
    .groupby("issue_year")["inq_fi"]
    .apply(lambda x: x.isna().mean())
)

# loan volume
loan_counts = accepted.groupby("issue_year").size()

#plot
fig, ax1 = plt.subplots()

line1, = ax1.plot(
    missing_rate_by_year,
    marker="o",
    label="Missingness Rate"
)
ax1.set_ylabel("Missingness Rate")
ax1.set_xlabel("Issue Year")

ax2 = ax1.twinx()
line2, = ax2.plot(
    loan_counts,
    linestyle="--",
    label="Loan Count"
)
ax2.set_ylabel("Loan Count")

# legend
lines = [line1, line2]
labels = [line.get_label() for line in lines]
ax1.legend(
    lines,
    labels,
    loc="center left"
)

ax1.set_title(
    "Missingness Rate of Credit Utilization Variables vs Loan Volume by Year")
plt.show()
```

From this graph, we can see that most of the loans in the dataset were 
issued from the period of 2014-2018, and the credit utilization data was 
only collected from 2016 onwards. This insight will guide modeling 
decisions later.

#### Employment and Income Variables

```{python}
cols = ["emp_title", "emp_length", "annual_inc"]

missing_summary = (
    accepted[cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
)

print(missing_summary)
```

There are only 4 missing values for `annual_inc` and only 6.5% of rows 
are missing `emp_length`. We will safely drop the rows where either of the 
two are missing, as our sample size is still large enough and there is 
such a small proportion of observations missing these values. We will also 
drop the `emp_title` variable entirely, as it will not be useful for 
modeling.

```{python}
# drop emp_title column
accepted = accepted.drop(columns=["emp_title"])

# drop rows where emp_length is missing
accepted = accepted.dropna(subset=["emp_length"])

# drop rows where annual_inc is missing
accepted = accepted.dropna(subset=["annual_inc"])
```

#### Credit Account Counts and Balances

```{python}
cols = [
    "num_tl_120dpd_2m",
    "mo_sin_old_il_acct",
    "bc_util",
    "percent_bc_gt_75",
    "bc_open_to_buy",
    "mths_since_recent_bc",
    "pct_tl_nvr_dlq",
    "avg_cur_bal",
    "mo_sin_rcnt_rev_tl_op",
    "num_rev_accts",
    "mo_sin_old_rev_tl_op",
    "num_op_rev_tl",
    "tot_coll_amt",
    "num_actv_rev_tl",
    "total_rev_hi_lim",
    "tot_cur_bal",
    "num_tl_30dpd",
    "num_actv_bc_tl",
    "num_accts_ever_120_pd",
    "num_rev_tl_bal_gt_0",
    "mo_sin_rcnt_tl",
    "num_tl_90g_dpd_24m",
    "tot_hi_cred_lim",
    "total_il_high_credit_limit",
    "num_il_tl",
    "num_tl_op_past_12m",
    "num_bc_tl",
    "num_bc_sats",
    "num_sats",
    "total_bal_ex_mort",
    "mort_acc",
    "acc_open_past_24mths",
    "total_bc_limit"
]

missing_summary = (
    accepted[cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

missing_summary
```

For `num_tl_120dpd_2m` and `mo_sin_old_il_acct`, missingness indicates 
0 accounts overdue and no prior installment loan, respectively. We can 
missing values with 0 for the former, and we'll create a flag variable 
for the latter while also filling missing values with a neutral value. 
This preserves these observations while maintaining interpretability 
for modeling.

```{python}
accepted["num_tl_120dpd_2m"] = (
    accepted["num_tl_120dpd_2m"]
    .fillna(0)
)

# create flag variable for presence/absence of prior installment loan
accepted["mo_sin_old_il_acct_missing"] = (
    accepted["mo_sin_old_il_acct"].isna().astype(int)
)

# fill missing values with neutral value of 0
accepted["mo_sin_old_il_acct"] = (
    accepted["mo_sin_old_il_acct"]
    .fillna(0)
)

```

We'll check if the missing values of all the variables with exactly 68,211 
missing values are conincident.

```{python}
# columns with exactly 68,211 missing values
cols_68211 = accepted.isna().sum().eq(68211)

# subset to those columns
cols_68211 = cols_68211[cols_68211].index

# check for coincidence
is_coincident = accepted[cols_68211].isna().all(axis=1).sum() == 68211

print(is_coincident)
```

The missingness of these variables is coincident, and since they only make 
up ~3% of observations, we will drop these from the dataset for simplicity.

```{python}
cols_68211 = accepted.isna().sum().eq(68211)
cols_68211 = cols_68211[cols_68211].index

# Drop rows where all of these variables are missing
accepted = accepted.dropna(subset=cols_68211)
```


```{python}
cols = [
    "bc_util", 
    "percent_bc_gt_75", 
    "bc_open_to_buy", 
    "mths_since_recent_bc", 
    "pct_tl_nvr_dlq", 
    "avg_cur_bal", 
    "num_rev_accts"]

missing_summary = (
    accepted[cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

missing_summary
```

For the remaining variables, missingness can be explained in the 
following ways:

-`bc_util`: no bankcard, can be filled with 0s
-`percent_bc_gt_75`: no bankcard, can be filled with 0s
-`bc_open_to_buy`: no bankcard, can be filled with 0s
-`mths_since_recent_bc`: no bankcard, can be filled with 0s
-`pct_tl_nvr_dlq`: data collection error, can be dropped 
-`avg_cur_bal`: data collection error, can be dropped 
-`num_rev_accts`: data collection error, can be dropped 

```{python}
# fill 0s for no bankcard variables
credit_zero_cols = [
    "bc_util",
    "percent_bc_gt_75",
    "bc_open_to_buy",
    "mths_since_recent_bc"
]
accepted[credit_zero_cols] = accepted[credit_zero_cols].fillna(0)

# drop rows for data collection errors
accepted = accepted.dropna(
    subset=["pct_tl_nvr_dlq", "avg_cur_bal", "num_rev_accts"]
)
```

#### Payment and Outcome Variables (Post-Origination)

```{python}
cols = [
    "last_pymnt_d",
    "next_pymnt_d",
    "last_pymnt_amnt",
    "recoveries",
    "collection_recovery_fee",
    "total_rec_prncp",
    "total_rec_int",
    "total_rec_late_fee"
]

missing_summary = (
    accepted[cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

print(missing_summary)
```

Where `next_pyment_d` is missing, this indicates a completed loan. 
`last_pyment_d` missing likely indicates a newly issued load (as of the 
publication of the data), so these will be dropped since they will not be 
useful for modeling. The rest of the payment variables do not have any 
missing values.

```{python}
# drop rows where next and last payment is missing
accepted = accepted.drop(columns=["next_pymnt_d", "last_pymnt_d"])
```

#### Administrative and Origination Variables

```{python}
cols = [
    "zip_code",
    "loan_amnt",
    "funded_amnt",
    "funded_amnt_inv",
    "term",
    "purpose",
    "issue_d",
    "loan_status"
]

missing_summary = (
    accepted[cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={"sum": "n_missing", "mean": "prop_missing"})
    .sort_values("prop_missing", ascending=False)
)

print(missing_summary)
```

`zip_code` will not be useful for modeling, so we can drop this column. We 
should look at the status of the loans in the dataset:

```{python}
accepted["loan_status"].value_counts().sort_index()
```

We will create a binary variable called `current_flag` to classify loans 
as current (1) or finished (0), meaning it was paid off or the borrower 
defaulted.

```{python}
# list of active statuses
active_statuses = [
    "Current",
    "In Grace Period",
    "Late (16-30 days)",
    "Late (31-120 days)"
]

# current = 1 if active, 0 if not
accepted["current_flag"] = (
    accepted["loan_status"].isin(active_statuses).astype(int))

```

We will also create a variable called `default`, where 1 means the 
borrower either defaulted or the loan was charged off, and 0 means 
the loan was either paid back in full or is still active (including late 
or in grace period).

```{python}
default_statuses = ["Charged Off", "Default"]

accepted["default"] = (
    accepted["loan_status"].isin(default_statuses).astype(int))
```

## Data Cleaning

```{python}
accepted["term"] = (
    accepted["term"]
    .str.replace(" months", "", regex=False)
    .astype(int)
)

accepted["emp_length"] = (
    accepted["emp_length"]
    .str.replace("+ years", "", regex=False)
    .astype(int)
)
```


```{python}
accepted["hardship_status"].value_counts(dropna=False).sort_index()
```