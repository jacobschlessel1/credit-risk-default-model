---
title: "Probability of Default (PD) Modeling"
author: "Jacob Schlessel"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Purpose

This notebook will perform feature selection, model building, and 
evaluate the performance of the Probability of Default (PD) model

## Setup

```{python}
import pandas as pd

df = pd.read_parquet(
    "../data/processed/accepted_clean.parquet",
    engine="pyarrow"
)

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

# Only use non-current loans
df = df.loc[df["current_flag"] == 0].copy()
```

This model will only consider non-current loans to ensure outcomes are 
fully realized.

## Feature selection

```{python}
# Define outcomes as default (=1)
y = df["default"]
y.value_counts(normalize=True)
```

Roughly 20% of loans in the dataset were defaulted. We will need to 
correct for this class imbalance in later modeling.

```{python}
# Categorical predictors
pred_cat = [
    "term", "region", "home_ownership"]

# Numeric predictor variables
pred_num = [

    # loan structure
    "loan_amnt",
  
    # borrower capacity
    "annual_inc",
    "emp_length",
    "dti",

    # credit quality
    "fico_range_low",
    "fico_range_high",
    "delinq_2yrs",
    "pub_rec",
    "pub_rec_bankruptcies",
    "collections_12_mths_ex_med",
    "chargeoff_within_12_mths",
    "tax_liens",

    # credit account info
    "revol_bal",
    "revol_util",
    "total_bal_ex_mort",
    "avg_cur_bal",
    "bc_util",
    "percent_bc_gt_75",
    "bc_open_to_buy",
    "total_rev_hi_lim",
    "total_bc_limit",
    "total_il_high_credit_limit",

    # credit depth & activity
    "open_acc",
    "total_acc",
    "mort_acc",
    "inq_last_6mths",
    "acc_open_past_24mths",
    "num_rev_accts",
    "num_il_tl",
    "num_op_rev_tl",
    "num_sats",


    # flag variables
    "joint_flag",
    "verified_flag",
    "verified_flag_joint",
    "whole_loan_flag",

    # structural missingness flags
    "mths_since_last_record_flag",
    "mths_since_recent_bc_dlq_flag",
    "mths_since_last_major_derog_flag",
    "mths_since_recent_revol_delinq_flag",
    "mths_since_last_delinq_flag",
]
```

### Interaction terms Based on Flag Variables

For some variables, we only want the model to consider their value based 
on the presence/absence of a corresponding flag variable. For example, 
for joint loans, we only want to consider the value of `annual_inc_joint` 
if the loan was filed jointly (`joint_flag` = 1). To achieve this, we 
will create interaction terms for all of the flag variables and their 
corresponding features. These will be included in linear models only. 

```{python}
# Conditional joint variables
joint_numeric_pred = [
    "sec_app_mths_since_last_major_derog",
    "sec_app_revol_util",
    "revol_bal_joint",
    "sec_app_open_acc",
    "sec_app_num_rev_accts",
    "sec_app_inq_last_6mths",
    "sec_app_mort_acc",
    "sec_app_open_act_il",
    "sec_app_fico_range_low",
    "sec_app_fico_range_high",
    "sec_app_chargeoff_within_12_mths",
    "sec_app_collections_12_mths_ex_med",
    "dti_joint",
    "annual_inc_joint"
]

joint_conditionals = []

for var in joint_numeric_pred:
    df[var] = df[var].fillna(0)
    df[f"{var}_active"] = df["joint_flag"] * df[var]
    joint_conditionals.append(f"{var}_active")

print(joint_conditionals)
```

```{python}
# Deliquency flags and interactions
flag_to_var = {
    "mths_since_last_record_flag": "mths_since_last_record",
    "mths_since_recent_bc_dlq_flag": "mths_since_recent_bc_dlq",
    "mths_since_last_major_derog_flag": "mths_since_last_major_derog",
    "mths_since_recent_revol_delinq_flag": "mths_since_recent_revol_delinq",
    "mths_since_last_delinq_flag": "mths_since_last_delinq",
}

interaction_delinq = []

for flag, var in flag_to_var.items():
    df[var] = df[var].replace(999, 0).fillna(0)
    df[f"{var}_active"] = df[flag] * df[var]
    interaction_delinq.append(f"{var}_active")

print(interaction_delinq)
```

We want to fill in all missing/previously filled in sentinel values with 0 
so that we can later standardize these variables for regression.


```{python}
df["post_2016"] = df["issue_d"].dt.year >= 2016

df["post_2016"].value_counts().rename({
    False: "pre_2016",
    True: "post_2016"
})

df_pre2016 = df[df["issue_d"].dt.year < 2016].copy()
df_post2016 = df[df["issue_d"].dt.year >= 2016].copy()
```

Roughly 2/3 of the loans are from before 2016, meaning these rows do not 
have data for any of the credit utilization variables. Because the sample 
size is large enough for both pre and post 2016, we will train two 
separate models: one for pre-2016 data without these variables, and one 
for post-2016 data including these variables as predictors.

### Notable Features Not Included

The following features will not be included as predictors because they are 
post-origination variables (would result in leakage) or because they are 
calculated using other features:

- `int_rate`: already contains information about riskiness of loan given 
the borrower's characteristics
- `grade`: calculated using other predictors
- all hardship variables: post-origination
- all settlement variables: post-origination
- Any payment timing variables: post origination
- Any recovery variables: post-origination


```{python}
# list of 2016 onwards credit utilization variables
recent_credit_vars_2016 = [
    "il_util",
    "mths_since_rcnt_il",
    "all_util",
    "open_acc_6m",
    "inq_last_12m",
    "total_cu_tl",
    "open_il_24m",
    "open_il_12m",
    "open_act_il",
    "max_bal_bc",
    "inq_fi",
    "total_bal_il",
    "open_rv_24m",
    "open_rv_12m"
]
```

The logistic regression model will include the joint interaction variables 
and the delinquency interaction variables as predictors, but other models 
will not include these.

## Modeling

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    roc_auc_score, confusion_matrix, classification_report)
from xgboost import XGBClassifier
import shap

def print_model_output(name, model, X_test, y_test):
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba >= 0.5).astype(int)

    auc = roc_auc_score(y_test, y_pred_proba)
    cm = confusion_matrix(y_test, y_pred)
    cr = classification_report(y_test, y_pred, digits=4)

    print(f"{name} AUC: {auc:.4f}")

    print("\nConfusion Matrix:")
    print(cm)

    print("\nClassification Report:")
    print(cr)

# Function to dummy encode categorical variables
def encode_categoricals(X):
    X = pd.get_dummies(
        X,
        columns=pred_cat,
        drop_first=True
    )

    return X

# Function to split data into training/testing subsets
def split_xy(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        stratify=y,
        random_state=42
    )

    return X_train, X_test, y_train, y_test


# Function to standardize all non-flag variables
def standardize_non_flags(X_train, X_test, scale_vars):
    scaler = StandardScaler()
    X_train[scale_vars] = scaler.fit_transform(X_train[scale_vars])
    X_test[scale_vars] = scaler.transform(X_test[scale_vars])

    return X_train, X_test

# Function to fit logitistic regression models
def fit_logit_model(X_train, y_train):
    model = LogisticRegression(
        penalty="l2", # regularized
        solver="lbfgs",
        max_iter=5000,
        class_weight="balanced",
        random_state=42
    )

    model.fit(X_train, y_train)

    return model

# Function to fit random forest models
def fit_rf_model(X_train, y_train):
    model = RandomForestClassifier(
        n_estimators=400,
        max_depth=None,
        min_samples_leaf=50,
        class_weight="balanced",
        n_jobs=-1,
        random_state=42
    )

    model.fit(X_train, y_train)

    return model

# Function to fit XGBoost models
def fit_xgb_model(X_train, y_train, n_estimators):
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    model = XGBClassifier(
        n_estimators=n_estimators,
        max_depth=4,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="binary:logistic",
        eval_metric="auc",
        scale_pos_weight=scale_pos_weight,
        tree_method="hist",
        n_jobs=-1,
        random_state=42
    )

    model.fit(X_train, y_train)

    return model
    
# Function to print SHAP values for XGBoost models
def print_top_shap_xgb(name, model, X, top_n=5):
    
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    shap_importance = pd.Series(
        np.abs(shap_values).mean(axis=0),
        index=X.columns
    ).sort_values(ascending=False)

    print(f"\n{name} Top {top_n} SHAP Features:")
    print(shap_importance.head(top_n))
```



### Pre-2016

```{python}
# PRE-2016 modeling

# define predictors

pred_num_final = (
    pred_num
    + joint_conditionals
    + interaction_delinq
)

# subset data

X = df_pre2016[pred_cat + pred_num_final].copy()
y = df_pre2016["default"].copy()

# encode categoricals

X = encode_categoricals(X)

# train/test split

X_train, X_test, y_train, y_test = split_xy(
    X,
    y
)
```

#### Logistic Regression

```{python}
# Pre-2016 logistic regression

scale_vars = [
    c for c in pred_num_final
    if not c.endswith("_flag") and not c.endswith("_missing")
]

X_train_logit, X_test_logit = standardize_non_flags(
    X_train.copy(),
    X_test.copy(),
    scale_vars
)

logit_pre2016 = fit_logit_model(
    X_train_logit,
    y_train
)

print("\nPRE-2016 LOGISTIC REGRESSION")
print_model_output(
    "Pre-2016 Logistic Regression",
    logit_pre2016,
    X_test_logit,
    y_test
)
```

#### Random Forest

```{python}
# Pre-2016 Random Forest

rf_pre2016 = fit_rf_model(
    X_train,
    y_train
)

print("\nPRE-2016 RANDOM FOREST")
print_model_output(
    "Pre-2016 Random Forest",
    rf_pre2016,
    X_test,
    y_test
)
```

#### XGBoost

```{python}
# Pre-2016 XGBoost

xgb_pre2016 = fit_xgb_model(
    X_train,
    y_train,
    n_estimators=400
)

print("\nPRE-2016 XGBOOST")
print_model_output(
    "Pre-2016 XGBoost",
    xgb_pre2016,
    X_test,
    y_test
)

print_top_shap_xgb(
    "Pre XGBoost",
    xgb_pre2016,
    X_train
)
```

The ROC-AUC values were very similar for all 3 pre-2016 models (~0.72), 
meaning each model performed decently at classifying default risk. 
Technically speaking, if you were to random pick out one defaulted loan 
and one non-defaulted loan, the models will assign a higher predicted probablity of default to the defaulted loan about 72% of the time.

The XGBoost model had the highest recall for defaulted loans (0.65 
compared to 0.56 and 0.63 for logistic regression and random forest 
respectively), meaning that it captured 65% of true defaults, and 
those that it did classify as likely to default were 35% accurate. In 
deploying this model, there are higher costs associated with false 
negatives and lower costs with false positives, so I would recommend 
the XGBoost model over the other 2 because it is less conservative with 
labeling borrowers as risky.

The most important features in predicting risk of default were the 
term rate of the loan, the number of accounts opened in the past 2 years, 
the low range of the borrower's FICO score, the debt-to-income ratio of 
the borrower, and the amount of the loan. Of these, interest rate was by 
far the most impactful predictor.


### Logistic Regression

### Post-2016:

```{python}
# Post 2016 modeling


# predictors: include credit variables
pred_num_final = (
    pred_num
    + joint_conditionals
    + interaction_delinq
    + recent_credit_vars_2016
)

# subset data
X = df_post2016[pred_cat + pred_num_final].copy()
y = df_post2016["default"].copy()

# fill missing credit variables with 0
for col in recent_credit_vars_2016:
    X[col] = X[col].fillna(0)

# encode categoricals
X = encode_categoricals(X)

# train/test split
X_train, X_test, y_train, y_test = split_xy(
    X,
    y
)
```

#### Logistic Regression

```{python}
# Post-2016 Logistic Regression

scale_vars = [
    c for c in X_train.columns
    if (
        c in pred_num_final
        and not c.endswith("_flag")
        and not c.endswith("_missing")
    )
]

X_train_logit, X_test_logit = standardize_non_flags(
    X_train.copy(),
    X_test.copy(),
    scale_vars
)

logit_post2016 = fit_logit_model(
    X_train_logit,
    y_train
)

print("\nPOST-2016 LOGISTIC REGRESSION")
print_model_output(
    "Post-2016 Logistic Regression",
    logit_post2016,
    X_test_logit,
    y_test
)
```

#### Random Forest

```{python}
# Post-2016 Random Forest

rf_post2016 = fit_rf_model(
    X_train,
    y_train
)

print("\nPOST-2016 RANDOM FOREST")
print_model_output(
    "Post-2016 Random Forest",
    rf_post2016,
    X_test,
    y_test
)
```

#### XGBoost

```{python}
# Post-2016 XGBoost

xgb_post2016 = fit_xgb_model(
    X_train,
    y_train,
    n_estimators=500
)

print("\nPOST-2016 XGBOOST")
print_model_output(
    "Post-2016 XGBoost",
    xgb_post2016,
    X_test,
    y_test
)

print_top_shap_xgb(
    "Post-2016 XGBoost",
    xgb_post2016,
    X_train
)
```

The ROC-AUC values for the post-2016 loans (loans with credit utilization 
date available) were just slightly higher than the pre-2016 loans (~0.73 
compared to 0.72). Once again, the XGBoost model captured more true 
defaults compared to the logistic regression and random forest models, 
however the recall and precision were almost exactly the same as the 
pre-2016 XGBoost model.

The most important features were almost exactly the same as the pre-2016 
model, though this time the number of mortgage accounts the borrower had 
was one of the top five most important predictors instead of the number 
of accounts opened in the past 2 years.

